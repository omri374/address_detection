{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "from presidio_evaluator import InputSample\n",
    "from presidio_evaluator.models.crf_model import CRFModel\n",
    "from presidio_evaluator.validation import split_dataset, save_to_json\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "DATA_PATH = \"../../presidio-research/data/generated_address_size_1000_date_November_07_2021.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baslines\n",
    "* CRF (conditional Random Field)\n",
    "* spacy (Evaluate with existing Spacy model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Split\n",
    "Note that we don't want the same pattern to be in more than one set. ([code sample](https://github.com/microsoft/presidio-research/blob/master/notebooks/Split%20by%20pattern%20%23.ipynb))    \n",
    "note that `split_dataset` function is based on `template#` in `meta_data` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "all_samples = read_synth_dataset(DATA_PATH)\n",
    "print(len(all_samples))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "all_samples[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Full text: X-Folder: \\Vincent_Kaminski_Jun2001_9\\Notes Folders\\Audit\n",
       "X-Origin: Kaminski-V\n",
       "X-FileName: vkamins.nsf\n",
       "\n",
       "Professor Darrel Duffie\n",
       "24201 North Highway One\n",
       "\n",
       "         May 8, 2000\n",
       "Spans: [Type: LOCATION, value: 24201 North Highway One, start: 128, end: 151]\n",
       "Tokens: [X, -, Folder, :, \\Vincent_Kaminski_Jun2001_9\\Notes, Folders\\Audit, \n",
       ", X, -, Origin, :, Kaminski, -, V, \n",
       ", X, -, FileName, :, vkamins.nsf, \n",
       "\n",
       ", Professor, Darrel, Duffie, \n",
       ", 24201, North, Highway, One, \n",
       "\n",
       "         , May, 8, ,, 2000]\n",
       "Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "TRAIN_TEST_RATIOS = [0.7,0.3]\n",
    "train_data,test_data = split_dataset(all_samples, TRAIN_TEST_RATIOS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "train_data[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Full text: X-FileName: kward.nsf\n",
       "\n",
       "Sure, here it is.\n",
       "\n",
       "Basin Electric Power Cooperative\n",
       "359 Alfred Avenue\n",
       "Teaneck, NJ 07666\n",
       "\n",
       "Tanya\n",
       "Spans: [Type: LOCATION, value: 359 Alfred Avenue\n",
       "Teaneck, NJ 07666, start: 75, end: 110]\n",
       "Tokens: [X, -, FileName, :, kward.nsf, \n",
       "\n",
       ", Sure, ,, here, it, is, ., \n",
       "\n",
       ", Basin, Electric, Power, Cooperative, \n",
       ", 359, Alfred, Avenue, \n",
       ", Teaneck, ,, NJ, 07666, \n",
       "\n",
       ", Tanya]\n",
       "Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CRF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# all_samples = read_synth_dataset(DATA_PATH)\n",
    "# all_samples = [sample for sample in all_samples if len(sample.spans) > 0]\n",
    "# print(\"Kept {} samples after removal of non-tagged samples\".format(len(all_samples)))\n",
    "\n",
    "# random.shuffle(all_samples)\n",
    "\n",
    "# train_len = int(len(all_samples)* 0.80)\n",
    "# train_data = all_samples[:train_len]\n",
    "# test_data = all_samples[train_len:]\n",
    "\n",
    "train_data = InputSample.create_conll_dataset(train_data, to_bio=False)\n",
    "test_data = InputSample.create_conll_dataset(test_data, to_bio=False)\n",
    "\n",
    "test_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regards</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNPS</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>_SP</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dave</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text    pos   tag label  sentence\n",
       "0     Best    ADJ    JJ     O         0\n",
       "1  Regards  PROPN  NNPS     O         0\n",
       "2        ,  PUNCT     ,     O         0\n",
       "3       \\n  SPACE   _SP     O         0\n",
       "4     Dave  PROPN   NNP     O         0"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Turn every sentence into a list of lists (list of tokens + pos + label)\n",
    "test_sents=test_data.groupby('sentence')[['text','pos','label']].apply(lambda x: x.values.tolist())\n",
    "train_sents=train_data.groupby('sentence')[['text','pos','label']].apply(lambda x: x.values.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "CRFModel.sent2features(train_sents[0])[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'word.lower()': 'sally',\n",
       " 'word[-3:]': 'lly',\n",
       " 'word[-2:]': 'ly',\n",
       " 'word.isupper()': False,\n",
       " 'word.istitle()': True,\n",
       " 'word.isdigit()': False,\n",
       " 'postag': 'PROPN',\n",
       " 'postag[:2]': 'PR',\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': ',',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'PUNCT',\n",
       " '+1:postag[:2]': 'PU'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%time\n",
    "X_train = [CRFModel.sent2features(s) for s in train_sents]\n",
    "y_train = [CRFModel.sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [CRFModel.sent2features(s) for s in test_sents]\n",
    "y_test = [CRFModel.sent2labels(s) for s in test_sents]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.53 µs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.42 s, sys: 2.21 ms, total: 3.42 s\n",
      "Wall time: 3.42 s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import pickle\n",
    "with open(\"../model_weights/crf.pickle\",'wb') as f:\n",
    "    pickle.dump(crf, f,protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "with open(\"../model_weights/crf.pickle\", 'rb') as f:\n",
    "    crf = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "labels = list(crf.classes_)\n",
    "# labels.remove('O')\n",
    "labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['O', 'B-GPE', 'I-GPE', 'L-GPE', 'U-GPE']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9742526374999367"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "y_5_pred = crf.predict([X_test[5]])\n",
    "# y_5_pred[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.994     0.977     0.985     17487\n",
      "       B-GPE      0.922     0.902     0.912       368\n",
      "       I-GPE      0.883     0.977     0.928      2933\n",
      "       L-GPE      0.880     0.878     0.879       368\n",
      "       U-GPE      1.000     0.333     0.500         3\n",
      "\n",
      "    accuracy                          0.974     21159\n",
      "   macro avg      0.936     0.814     0.841     21159\n",
      "weighted avg      0.975     0.974     0.974     21159\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['O', 'B-GPE', 'I-GPE', 'L-GPE', 'U-GPE'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate an existing spaCy trained model\n",
    "* Using [this noteboook](https://github.com/microsoft/presidio-research/blob/master/notebooks/models/Evaluate%20spacy%20models.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from presidio_evaluator.models import SpacyModel\n",
    "\n",
    "from presidio_evaluator.evaluation import Evaluator, ModelError\n",
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "synth_samples = read_synth_dataset(DATA_PATH)\n",
    "print(len(synth_samples))\n",
    "DATASET = synth_samples"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "entity_counter = Counter()\n",
    "for sample in DATASET:\n",
    "    for span in sample.spans:\n",
    "        entity_counter[span.entity_type]+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "entity_counter"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'LOCATION': 1250})"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "DATASET[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Full text: * Asia Pacific Energy - April 18 \n",
       "\n",
       "Location and Accommodations: \n",
       "\n",
       "The Four Seasons Hotel \n",
       "19191 Vallco Pkwy , Cupertino  200 Forest Street\n",
       "Tel.: +1 713 650 1300 \n",
       "Fax: +1 713 650 1203 \n",
       "* Please contact the hotel directly for room reservations.\n",
       "Spans: [Type: LOCATION, value: 19191 Vallco Pkwy , Cupertino  200 Forest Street, start: 90, end: 138]\n",
       "Tokens: [*, Asia, Pacific, Energy, -, April, 18, \n",
       "\n",
       ", Location, and, Accommodations, :, \n",
       "\n",
       ", The, Four, Seasons, Hotel, \n",
       ", 19191, Vallco, Pkwy, ,, Cupertino,  , 200, Forest, Street, \n",
       ", Tel, ., :, +1, 713, 650, 1300, \n",
       ", Fax, :, +1, 713, 650, 1203, \n",
       ", *, Please, contact, the, hotel, directly, for, room, reservations, .]\n",
       "Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#max length sentence\n",
    "max([len(sample.tokens) for sample in DATASET])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "models = [\n",
    "    # \"en_core_web_lg\", \n",
    "    \"en_core_web_trf\",\n",
    "    ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for model in models:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Evaluating model {}\".format(model))\n",
    "    nlp = spacy.load(model)\n",
    "    spacy_model = SpacyModel(model=nlp,entities_to_keep=['GPE'])\n",
    "    evaluator = Evaluator(model=spacy_model)\n",
    "    evaluation_results = evaluator.evaluate_all(DATASET)\n",
    "    scores = evaluator.calculate_score(evaluation_results)\n",
    "    \n",
    "    print(\"Confusion matrix:\")\n",
    "    print(scores.results)\n",
    "\n",
    "    print(\"Precision and recall\")\n",
    "    scores.print()\n",
    "    errors = scores.model_errors"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------------------------------\n",
      "Evaluating model en_core_web_trf\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating <class 'presidio_evaluator.evaluation.evaluator.Evaluator'>:   0%|          | 0/1000 [00:00<?, ?it/s]/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "Evaluating <class 'presidio_evaluator.evaluation.evaluator.Evaluator'>:   0%|          | 2/1000 [00:00<01:22, 12.04it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translating entites using this dictionary: {'ORGANIZATION': 'ORG', 'COUNTRY': 'GPE', 'CITY': 'GPE', 'LOCATION': 'GPE', 'PERSON': 'PERSON', 'FIRST_NAME': 'PERSON', 'LAST_NAME': 'PERSON', 'NATION_MAN': 'GPE', 'NATION_WOMAN': 'GPE', 'NATION_PLURAL': 'GPE', 'NATIONALITY': 'GPE', 'GPE': 'GPE', 'ORG': 'ORG'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating <class 'presidio_evaluator.evaluation.evaluator.Evaluator'>: 100%|██████████| 1000/1000 [01:48<00:00,  9.26it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion matrix:\n",
      "Counter({('O', 'O'): 61584, ('GPE', 'O'): 10365, ('GPE', 'GPE'): 1707, ('O', 'GPE'): 480})\n",
      "Precision and recall\n",
      "                        Entity                     Precision                        Recall\n",
      "                           GPE                        78.05%                        14.14%\n",
      "                           PII                        78.05%                        14.14%\n",
      "PII F measure: 0.15940530932416241\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare training data for spacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# train_data\n",
    "train_tagged = [sample for sample in train_data if len(sample.spans)>0]\n",
    "print(\"Kept {} samples after removal of non-tagged samples(for train)\".format(len(train_tagged)))\n",
    "\n",
    "test_tagged = [sample for sample in test_data if len(sample.spans)>0]\n",
    "print(\"Kept {} samples after removal of non-tagged samples(for test)\".format(len(test_tagged)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Entities found in training set:\")\n",
    "entities = []\n",
    "for sample in train_tagged:\n",
    "    entities.extend([tag for tag in sample.tags])\n",
    "set(entities)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kept 704 samples after removal of non-tagged samples(for train)\n",
      "Kept 296 samples after removal of non-tagged samples(for test)\n",
      "Entities found in training set:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'B-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'U-LOCATION'}"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import DocBin\n",
    "import os\n",
    "\n",
    "SPACY_ROOT_PATH = \"../data/spacy/\"\n",
    "\n",
    "if not os.path.exists(SPACY_ROOT_PATH):\n",
    "    os.mkdir(SPACY_ROOT_PATH)\n",
    "\n",
    "def convert_df_to_spacy(samples, file_name):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # the DocBin will store the example documents\n",
    "    db = DocBin()\n",
    "    for sample in samples:\n",
    "        \n",
    "        text = sample.full_text\n",
    "        doc = nlp(text)\n",
    "        ents = []\n",
    "        annotations =[ (item.start_position, item.end_position) for item in sample.spans]\n",
    "        # print(i)\n",
    "        # print(annotations)\n",
    "        for start, end in annotations:\n",
    "            \n",
    "            span = doc.char_span(start, end, label=\"GPE\", alignment_mode=\"expand\")\n",
    "            # print(span)\n",
    "            ents.append(span)\n",
    "        \n",
    "        #to solve overlaping entites (drops them)\n",
    "        ents = filter_spans(ents)\n",
    "        \n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(f\"{SPACY_ROOT_PATH}/{file_name}\")\n",
    "    print(f\"{file_name} successfully saved!\")\n",
    "\n",
    "convert_df_to_spacy(train_tagged,\"train.spacy\")\n",
    "convert_df_to_spacy(test_tagged,\"dev.spacy\")\n",
    "# convert_df_to_spacy(df_test,\"test.spacy\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train.spacy successfully saved!\n",
      "dev.spacy successfully saved!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "training a spacy model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. `python -m spacy init fill-config base_config.cfg config.cfg`\n",
    "2.  `python -m spacy train config/config.cfg --output ./output --paths.train ./data/spacy/train.spacy --paths.dev ./data/spacy/dev.spacy`\n",
    "3. `python -m spacy evaluate \"./output/model-best\" \"./data/spacy/test.spacy\"`"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('address': virtualenv)"
  },
  "interpreter": {
   "hash": "44acb1839da533283325c49d8a2ac7bf01c05426dc88dc963dbf3f573e808931"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}