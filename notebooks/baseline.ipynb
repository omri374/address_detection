{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "from presidio_evaluator import InputSample\n",
    "from presidio_evaluator.models.crf_model import CRFModel\n",
    "from presidio_evaluator.validation import split_dataset, save_to_json\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_PATH = \"../../presidio-research/data/generated_address_size_2000_date_October_27_2021.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baslines\n",
    "* CRF (conditional Random Field)\n",
    "* spacy (Evaluate with existing Spacy model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Split\n",
    "Note that we don't want the same pattern to be in more than one set. ([code sample](https://github.com/microsoft/presidio-research/blob/master/notebooks/Split%20by%20pattern%20%23.ipynb))    \n",
    "note that `split_dataset` function is based on `template#` in `meta_data` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_samples = read_synth_dataset(DATA_PATH)\n",
    "print(len(all_samples))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_samples[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRAIN_TEST_RATIOS = [0.7,0.3]\n",
    "train,test = split_dataset(all_samples, TRAIN_TEST_RATIOS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CRF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_samples = read_synth_dataset(DATA_PATH)\n",
    "all_samples = [sample for sample in all_samples if len(sample.spans) > 0]\n",
    "print(\"Kept {} samples after removal of non-tagged samples\".format(len(all_samples)))\n",
    "\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "train_len = int(len(all_samples)* 0.80)\n",
    "train_data = all_samples[:train_len]\n",
    "test_data = all_samples[train_len:]\n",
    "\n",
    "train_data = InputSample.create_conll_dataset(train_data, to_bio=False)\n",
    "test_data = InputSample.create_conll_dataset(test_data, to_bio=False)\n",
    "\n",
    "test_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Turn every sentence into a list of lists (list of tokens + pos + label)\n",
    "test_sents=test_data.groupby('sentence')[['text','pos','label']].apply(lambda x: x.values.tolist())\n",
    "train_sents=train_data.groupby('sentence')[['text','pos','label']].apply(lambda x: x.values.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CRFModel.sent2features(train_sents[0])[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%time\n",
    "X_train = [CRFModel.sent2features(s) for s in train_sents]\n",
    "y_train = [CRFModel.sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [CRFModel.sent2features(s) for s in test_sents]\n",
    "y_test = [CRFModel.sent2labels(s) for s in test_sents]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "with open(\"../model_weights/crf.pickle\",'wb') as f:\n",
    "    pickle.dump(crf, f,protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"../model_weights/crf.pickle\", 'rb') as f:\n",
    "    crf = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = list(crf.classes_)\n",
    "# labels.remove('O')\n",
    "labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_5_pred = crf.predict([X_test[5]])\n",
    "# y_5_pred[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate an existing spaCy trained model\n",
    "* Using [this noteboook](https://github.com/microsoft/presidio-research/blob/master/notebooks/models/Evaluate%20spacy%20models.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from presidio_evaluator.models import SpacyModel\n",
    "\n",
    "from presidio_evaluator.evaluation import Evaluator, ModelError\n",
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "synth_samples = read_synth_dataset(\"../../presidio-research/data/generated_address_size_2000_date_October_27_2021.json\")\n",
    "print(len(synth_samples))\n",
    "DATASET = synth_samples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entity_counter = Counter()\n",
    "for sample in DATASET:\n",
    "    for span in sample.spans:\n",
    "        entity_counter[span.entity_type]+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "entity_counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATASET[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#max length sentence\n",
    "max([len(sample.tokens) for sample in DATASET])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "models = [\n",
    "    \"en_core_web_lg\", \n",
    "    # \"en_core_web_trf\",\n",
    "    ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for model in models:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Evaluating model {}\".format(model))\n",
    "    nlp = spacy.load(model)\n",
    "    spacy_model = SpacyModel(model=nlp,entities_to_keep=['GPE'])\n",
    "    evaluator = Evaluator(model=spacy_model)\n",
    "    evaluation_results = evaluator.evaluate_all(DATASET)\n",
    "    scores = evaluator.calculate_score(evaluation_results)\n",
    "    \n",
    "    print(\"Confusion matrix:\")\n",
    "    print(scores.results)\n",
    "\n",
    "    print(\"Precision and recall\")\n",
    "    scores.print()\n",
    "    errors = scores.model_errors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('address': virtualenv)"
  },
  "interpreter": {
   "hash": "44acb1839da533283325c49d8a2ac7bf01c05426dc88dc963dbf3f573e808931"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}