{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "from presidio_evaluator import InputSample\n",
    "from presidio_evaluator.models.crf_model import CRFModel\n",
    "from presidio_evaluator.validation import split_dataset, save_to_json\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Flair is not installed by default\n",
      "Flair is not installed\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "DATA_PATH = \"../../presidio-research/data/generated_address_size_1000_date_November_07_2021.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baslines\n",
    "* CRF (conditional Random Field)\n",
    "* spacy (Evaluate with existing Spacy model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Split\n",
    "Note that we don't want the same pattern to be in more than one set. ([code sample](https://github.com/microsoft/presidio-research/blob/master/notebooks/Split%20by%20pattern%20%23.ipynb))    \n",
    "note that `split_dataset` function is based on `template#` in `meta_data` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "all_samples = read_synth_dataset(DATA_PATH)\n",
    "print(len(all_samples))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "all_samples[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Full text: X-Folder: \\Vincent_Kaminski_Jun2001_9\\Notes Folders\\Audit\n",
       "X-Origin: Kaminski-V\n",
       "X-FileName: vkamins.nsf\n",
       "\n",
       "Professor Darrel Duffie\n",
       "24201 North Highway One\n",
       "\n",
       "         May 8, 2000\n",
       "Spans: [Type: LOCATION, value: 24201 North Highway One, start: 128, end: 151]\n",
       "Tokens: [X, -, Folder, :, \\Vincent_Kaminski_Jun2001_9\\Notes, Folders\\Audit, \n",
       ", X, -, Origin, :, Kaminski, -, V, \n",
       ", X, -, FileName, :, vkamins.nsf, \n",
       "\n",
       ", Professor, Darrel, Duffie, \n",
       ", 24201, North, Highway, One, \n",
       "\n",
       "         , May, 8, ,, 2000]\n",
       "Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "TRAIN_TEST_RATIOS = [0.7,0.3]\n",
    "train_data,test_data = split_dataset(all_samples, TRAIN_TEST_RATIOS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Full text: Subject: Margie Harris\n",
       "\n",
       "\n",
       "Margie Harris\n",
       "\n",
       "319 West 48th Street\n",
       "uturn 1400 Smith Street, EB4771 \n",
       "Houston, Tx 77001713) 853-7172 on right, lavander house\n",
       "Spans: [Type: LOCATION, value: 319 West 48th Street, start: 40, end: 60, Type: LOCATION, value: 1400 Smith Street, EB4771 \n",
       "Houston, Tx 77001713) 853-7172, start: 67, end: 124]\n",
       "Tokens: [Subject, :, Margie, Harris, \n",
       "\n",
       "\n",
       ", Margie, Harris, \n",
       "\n",
       ", 319, West, 48th, Street, \n",
       ", uturn, 1400, Smith, Street, ,, EB4771, \n",
       ", Houston, ,, Tx, 77001713, ), 853, -, 7172, on, right, ,, lavander, house]\n",
       "Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CRF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# all_samples = read_synth_dataset(DATA_PATH)\n",
    "# all_samples = [sample for sample in all_samples if len(sample.spans) > 0]\n",
    "# print(\"Kept {} samples after removal of non-tagged samples\".format(len(all_samples)))\n",
    "\n",
    "# random.shuffle(all_samples)\n",
    "\n",
    "# train_len = int(len(all_samples)* 0.80)\n",
    "# train_data = all_samples[:train_len]\n",
    "# test_data = all_samples[train_len:]\n",
    "\n",
    "train_data_CRF = InputSample.create_conll_dataset(train_data, to_bio=False)\n",
    "test_data_CRF = InputSample.create_conll_dataset(test_data, to_bio=False)\n",
    "\n",
    "test_data_CRF.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wednesdays</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wo</td>\n",
       "      <td>AUX</td>\n",
       "      <td>MD</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n't</td>\n",
       "      <td>PART</td>\n",
       "      <td>RB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drive</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text    pos  tag label  sentence\n",
       "0  Wednesdays   NOUN  NNS     O         0\n",
       "1         and  CCONJ   CC     O         0\n",
       "2          wo    AUX   MD     O         0\n",
       "3         n't   PART   RB     O         0\n",
       "4       drive   VERB   VB     O         0"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Turn every sentence into a list of lists (list of tokens + pos + label)\n",
    "test_sents=test_data_CRF.groupby('sentence')[['text','pos','label']].apply(lambda x: x.values.tolist())\n",
    "train_sents=train_data_CRF.groupby('sentence')[['text','pos','label']].apply(lambda x: x.values.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "CRFModel.sent2features(train_sents[0])[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bias': 1.0,\n",
       " 'word.lower()': 'subject',\n",
       " 'word[-3:]': 'ect',\n",
       " 'word[-2:]': 'ct',\n",
       " 'word.isupper()': False,\n",
       " 'word.istitle()': True,\n",
       " 'word.isdigit()': False,\n",
       " 'postag': 'NOUN',\n",
       " 'postag[:2]': 'NO',\n",
       " 'BOS': True,\n",
       " '+1:word.lower()': ':',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:postag': 'PUNCT',\n",
       " '+1:postag[:2]': 'PU'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "%time\n",
    "X_train = [CRFModel.sent2features(s) for s in train_sents]\n",
    "y_train = [CRFModel.sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [CRFModel.sent2features(s) for s in test_sents]\n",
    "y_test = [CRFModel.sent2labels(s) for s in test_sents]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.63 s, sys: 3.69 ms, total: 3.63 s\n",
      "Wall time: 3.63 s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import pickle\n",
    "with open(\"../model_weights/crf.pickle\",'wb') as f:\n",
    "    pickle.dump(crf, f,protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import pickle\n",
    "with open(\"../model_weights/crf.pickle\", 'rb') as f:\n",
    "    crf = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "labels = list(crf.classes_)\n",
    "# labels.remove('O')\n",
    "labels"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['O', 'B-GPE', 'I-GPE', 'L-GPE', 'U-GPE']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9861090774209577"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "y_5_pred = crf.predict([X_test[5]])\n",
    "# y_5_pred[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.994     0.991     0.992     20714\n",
      "       B-GPE      0.941     0.939     0.940       358\n",
      "       I-GPE      0.948     0.968     0.958      2924\n",
      "       L-GPE      0.908     0.905     0.906       358\n",
      "       U-GPE      1.000     0.500     0.667         4\n",
      "\n",
      "    accuracy                          0.986     24358\n",
      "   macro avg      0.958     0.861     0.893     24358\n",
      "weighted avg      0.986     0.986     0.986     24358\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['O', 'B-GPE', 'I-GPE', 'L-GPE', 'U-GPE'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## predict on test set without newline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "test_data[0].full_text.replace(\"\\n\", \" \")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Wednesdays and won't drive back on I-40 if it's after 3:00, I take a back route.  Let me give you directions to Orton just in case, it's easy.  I-40 from Raleigh to Wilmington, as you approach Wilmington take a right at a big light on 600 Fifth Avenue Rockefeller Center - 27th floor New York, NY 10020 (new since your day, it's just before 10435 N Tantau Ave, Cupertino  Old-Bolton Rd.), go about 1-2 miles and it will end, take a right on 5959 Topanga Canyon Blvd.  Suite 244 Woodland Hills, CA 91367, pass the airport on your right, road will come to an end again, take a left, I think this is Hwy 133 South which takes you around/through nig-town Wilmington, cross the Cape Fear River and follow signs to Southport staying\""
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#remove new line\n",
    "test_data_no_newline = []\n",
    "for data_with_newline in test_data:\n",
    "    data_with_newline.full_text = data_with_newline.full_text.replace(\"\\n\", \" \")\n",
    "    test_data_no_newline.append(data_with_newline)\n",
    "\n",
    "#test\n",
    "test_data_CRF = InputSample.create_conll_dataset(test_data_no_newline, to_bio=False)\n",
    "test_sents=test_data_CRF.groupby('sentence')[['text','pos','label']].apply(lambda x: x.values.tolist())\n",
    "\n",
    "X_test = [CRFModel.sent2features(s) for s in test_sents]\n",
    "y_test = [CRFModel.sent2labels(s) for s in test_sents]\n",
    "\n",
    "import pickle\n",
    "with open(\"../model_weights/crf.pickle\", 'rb') as f:\n",
    "    crf = pickle.load(f)\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O      0.994     0.991     0.992     20714\n",
      "       B-GPE      0.941     0.939     0.940       358\n",
      "       I-GPE      0.948     0.968     0.958      2924\n",
      "       L-GPE      0.908     0.905     0.906       358\n",
      "       U-GPE      1.000     0.500     0.667         4\n",
      "\n",
      "    accuracy                          0.986     24358\n",
      "   macro avg      0.958     0.861     0.893     24358\n",
      "weighted avg      0.986     0.986     0.986     24358\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate an existing spaCy trained model\n",
    "* Using [this noteboook](https://github.com/microsoft/presidio-research/blob/master/notebooks/models/Evaluate%20spacy%20models.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from presidio_evaluator.models import SpacyModel\n",
    "\n",
    "from presidio_evaluator.evaluation import Evaluator, ModelError\n",
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "synth_samples = read_synth_dataset(DATA_PATH)\n",
    "print(len(synth_samples))\n",
    "DATASET = synth_samples"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "entity_counter = Counter()\n",
    "for sample in DATASET:\n",
    "    for span in sample.spans:\n",
    "        entity_counter[span.entity_type]+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "entity_counter"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'LOCATION': 1250})"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "DATASET[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Full text: * Asia Pacific Energy - April 18 \n",
       "\n",
       "Location and Accommodations: \n",
       "\n",
       "The Four Seasons Hotel \n",
       "19191 Vallco Pkwy , Cupertino  200 Forest Street\n",
       "Tel.: +1 713 650 1300 \n",
       "Fax: +1 713 650 1203 \n",
       "* Please contact the hotel directly for room reservations.\n",
       "Spans: [Type: LOCATION, value: 19191 Vallco Pkwy , Cupertino  200 Forest Street, start: 90, end: 138]\n",
       "Tokens: [*, Asia, Pacific, Energy, -, April, 18, \n",
       "\n",
       ", Location, and, Accommodations, :, \n",
       "\n",
       ", The, Four, Seasons, Hotel, \n",
       ", 19191, Vallco, Pkwy, ,, Cupertino,  , 200, Forest, Street, \n",
       ", Tel, ., :, +1, 713, 650, 1300, \n",
       ", Fax, :, +1, 713, 650, 1203, \n",
       ", *, Please, contact, the, hotel, directly, for, room, reservations, .]\n",
       "Tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "#max length sentence\n",
    "max([len(sample.tokens) for sample in DATASET])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "models = [\n",
    "    \"en_core_web_lg\", \n",
    "    \"en_core_web_trf\",\n",
    "    ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "for model in models:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Evaluating model {}\".format(model))\n",
    "    nlp = spacy.load(model)\n",
    "    spacy_model = SpacyModel(model=nlp,entities_to_keep=['GPE'])\n",
    "    evaluator = Evaluator(model=spacy_model)\n",
    "    evaluation_results = evaluator.evaluate_all(DATASET)\n",
    "    scores = evaluator.calculate_score(evaluation_results)\n",
    "    \n",
    "    print(\"Confusion matrix:\")\n",
    "    print(scores.results)\n",
    "\n",
    "    print(\"Precision and recall\")\n",
    "    scores.print()\n",
    "    errors = scores.model_errors"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-----------------------------------\n",
      "Evaluating model en_core_web_lg\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating <class 'presidio_evaluator.evaluation.evaluator.Evaluator'>:   1%|          | 10/1000 [00:00<00:10, 96.54it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translating entites using this dictionary: {'ORGANIZATION': 'ORG', 'COUNTRY': 'GPE', 'CITY': 'GPE', 'LOCATION': 'GPE', 'PERSON': 'PERSON', 'FIRST_NAME': 'PERSON', 'LAST_NAME': 'PERSON', 'NATION_MAN': 'GPE', 'NATION_WOMAN': 'GPE', 'NATION_PLURAL': 'GPE', 'NATIONALITY': 'GPE', 'GPE': 'GPE', 'ORG': 'ORG'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating <class 'presidio_evaluator.evaluation.evaluator.Evaluator'>: 100%|██████████| 1000/1000 [00:12<00:00, 79.95it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion matrix:\n",
      "Counter({('O', 'O'): 61596, ('GPE', 'O'): 10536, ('GPE', 'GPE'): 1536, ('O', 'GPE'): 468})\n",
      "Precision and recall\n",
      "                        Entity                     Precision                        Recall\n",
      "                           GPE                        76.65%                        12.72%\n",
      "                           PII                        76.65%                        12.72%\n",
      "PII F measure: 0.14377566039197456\n",
      "-----------------------------------\n",
      "Evaluating model en_core_web_trf\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Evaluating <class 'presidio_evaluator.evaluation.evaluator.Evaluator'>:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Translating entites using this dictionary: {'ORGANIZATION': 'ORG', 'COUNTRY': 'GPE', 'CITY': 'GPE', 'LOCATION': 'GPE', 'PERSON': 'PERSON', 'FIRST_NAME': 'PERSON', 'LAST_NAME': 'PERSON', 'NATION_MAN': 'GPE', 'NATION_WOMAN': 'GPE', 'NATION_PLURAL': 'GPE', 'NATIONALITY': 'GPE', 'GPE': 'GPE', 'ORG': 'ORG'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "Evaluating <class 'presidio_evaluator.evaluation.evaluator.Evaluator'>: 100%|██████████| 1000/1000 [02:07<00:00,  7.83it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion matrix:\n",
      "Counter({('O', 'O'): 61584, ('GPE', 'O'): 10365, ('GPE', 'GPE'): 1707, ('O', 'GPE'): 480})\n",
      "Precision and recall\n",
      "                        Entity                     Precision                        Recall\n",
      "                           GPE                        78.05%                        14.14%\n",
      "                           PII                        78.05%                        14.14%\n",
      "PII F measure: 0.15940530932416241\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare training data for spacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# train_data\n",
    "train_tagged = [sample for sample in train_data if len(sample.spans)>0]\n",
    "print(\"Kept {} samples after removal of non-tagged samples(for train)\".format(len(train_tagged)))\n",
    "\n",
    "test_tagged = [sample for sample in test_data if len(sample.spans)>0]\n",
    "print(\"Kept {} samples after removal of non-tagged samples(for test)\".format(len(test_tagged)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Entities found in training set:\")\n",
    "entities = []\n",
    "for sample in train_tagged:\n",
    "    entities.extend([tag for tag in sample.tags])\n",
    "set(entities)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Kept 683 samples after removal of non-tagged samples(for train)\n",
      "Kept 317 samples after removal of non-tagged samples(for test)\n",
      "Entities found in training set:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'B-LOCATION', 'I-LOCATION', 'L-LOCATION', 'O', 'U-LOCATION'}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import DocBin\n",
    "import os\n",
    "\n",
    "SPACY_ROOT_PATH = \"../data/spacy/\"\n",
    "\n",
    "if not os.path.exists(SPACY_ROOT_PATH):\n",
    "    os.mkdir(SPACY_ROOT_PATH)\n",
    "\n",
    "def convert_df_to_spacy(samples, file_name):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # the DocBin will store the example documents\n",
    "    db = DocBin()\n",
    "    for sample in samples:\n",
    "        \n",
    "        text = sample.full_text\n",
    "        # text = text.replace(\"\\n\", \" \")\n",
    "        doc = nlp(text)\n",
    "        ents = []\n",
    "        annotations =[ (item.start_position, item.end_position) for item in sample.spans]\n",
    "        # print(i)\n",
    "        # print(annotations)\n",
    "        for start, end in annotations:\n",
    "            \n",
    "            span = doc.char_span(start, end, label=\"GPE\", alignment_mode=\"expand\")\n",
    "            # print(span)\n",
    "            ents.append(span)\n",
    "        \n",
    "        #to solve overlaping entites (drops them)\n",
    "        ents = filter_spans(ents)\n",
    "        \n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    db.to_disk(f\"{SPACY_ROOT_PATH}/{file_name}\")\n",
    "    print(f\"{file_name} successfully saved!\")\n",
    "\n",
    "convert_df_to_spacy(train_tagged,\"train.spacy\")\n",
    "convert_df_to_spacy(test_tagged,\"dev.spacy\")\n",
    "# convert_df_to_spacy(df_test,\"test.spacy\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train.spacy successfully saved!\n",
      "dev.spacy successfully saved!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "training a spacy model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. `python -m spacy init fill-config base_config.cfg config.cfg`\n",
    "2.  `python -m spacy train config/config.cfg --output ./output --paths.train ./data/spacy/train.spacy --paths.dev ./data/spacy/dev.spacy`\n",
    "3. `python -m spacy evaluate \"./output/model-best\" \"./data/spacy/dev.spacy\"`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "! python -m spacy train ../config/config.cfg --output ../output --paths.train ../data/spacy/train.spacy --paths.dev ../data/spacy/dev.spacy"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-11-29 11:48:52,246] [INFO] Set up nlp object from config\n",
      "[2021-11-29 11:48:52,253] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-11-29 11:48:52,256] [INFO] Created vocabulary\n",
      "[2021-11-29 11:49:05,369] [INFO] Added vectors: en_core_web_lg\n",
      "[2021-11-29 11:49:05,369] [INFO] Finished initializing nlp object\n",
      "[2021-11-29 11:49:07,948] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     14.83    0.14    0.10    0.28    0.00\n",
      "  0     200        110.09   1235.15   39.35   47.83   33.43    0.39\n",
      "  0     400         20.76    301.69   54.83   57.40   52.49    0.55\n",
      "  1     600         59.53    313.94   49.36   50.73   48.07    0.49\n",
      "  2     800        132.63    281.00   62.11   67.20   57.73    0.62\n",
      "  2    1000         93.79    239.62   52.26   55.42   49.45    0.52\n",
      "  3    1200        102.72    222.44   67.28   75.78   60.50    0.67\n",
      "  5    1400        246.14    203.49   66.86   71.25   62.98    0.67\n",
      "  6    1600        284.49    195.76   68.07   69.03   67.13    0.68\n",
      "  8    1800        344.89    176.61   69.27   75.74   63.81    0.69\n",
      " 10    2000        132.68    155.94   73.48   73.48   73.48    0.73\n",
      " 13    2200        212.50    143.38   72.30   72.50   72.10    0.72\n",
      " 17    2400        201.44    124.78   70.30   73.13   67.68    0.70\n",
      " 21    2600        185.74    126.26   71.79   74.12   69.61    0.72\n",
      " 24    2800        226.45    104.41   70.54   70.64   70.44    0.71\n",
      " 28    3000        380.66    169.03   73.33   73.74   72.93    0.73\n",
      " 32    3200        387.86    111.02   74.79   75.85   73.76    0.75\n",
      " 36    3400        102.77     76.99   71.55   72.99   70.17    0.72\n",
      "^C\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "! python -m spacy evaluate \"../output/model-best\" \"../data/spacy/dev.spacy\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/fatemeh.rahimi/.virtualenvs/address/lib/python3.8/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "================================== Results ==================================\u001b[0m\n",
      "\n",
      "TOK     100.00\n",
      "NER P   75.85 \n",
      "NER R   73.76 \n",
      "NER F   74.79 \n",
      "SPEED   4958  \n",
      "\n",
      "\u001b[1m\n",
      "=============================== NER (per type) ===============================\u001b[0m\n",
      "\n",
      "          P       R       F\n",
      "GPE   75.85   73.76   74.79\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('address': virtualenv)"
  },
  "interpreter": {
   "hash": "44acb1839da533283325c49d8a2ac7bf01c05426dc88dc963dbf3f573e808931"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}