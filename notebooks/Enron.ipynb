{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extract addresses from the Enron dataset\n",
    "\n",
    "The [Enron Email Dataset](https://www.cs.cmu.edu/~enron/) was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes).\n",
    "It contains data from about 150 users, mostly senior management of Enron, organized into folders.\n",
    "The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation.\n",
    "\n",
    "This code uses simple regex to extract possible mailing addresses from the Enron dataset, for further inspection and manual tagging of addresses.\n",
    "\n",
    "While the simple regex approach would probably have false detection and missed addresses, it can help curate a labeled dataset of texts with and without addresses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5213e8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from typing import Tuple, Iterator, Dict\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial setup and detection logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_path = \"../data/maildir/\"\n",
    "email_regex = r\"^\\S+@\\S+\\.\\S+$\"\n",
    "\n",
    "street_hints = [\"st\\.\",\"street\",\"blvd\",\"boulevard\",\"rd\\.\",\"road\",\"ave\",\"avenue\",\"lane\",\"apt\\.\",\"apartment\",\"circle\",\"route\",\"ci\\.\",\"ct\\.\",\"court\",\"pkwy\",\"parkway\",\"freeway\",\"highway\",\"terrace\"]\n",
    "\n",
    "street_regex = f\"(?:^|(?<= ))({'|'.join(street_hints)})(?:(?=[ ?.,!])|$)\"\n",
    "\n",
    "# phrases used to ignore specific cases where the regex detected non-address matches\n",
    "street_phrases_to_ignore = [\"wall street\",\"st. mary\",\"st. peter\", \"st. john\",\"road conditions\",\n",
    "                            \"court to\",\"@\",\"federal court\",\"court of appeals\",\"supreme court\",\n",
    "                            \"st. lucia\",\"st.  lucia\",\"st. maarten\",\"st. regis\",\"st. clair\",\n",
    "                            \"st. louis\",\"st. thomas\",\"road runner\",\"road show\",\"to route\",\"claims court\"]\n",
    "street_phrases_to_ignore.extend([f\"a {word}\" for word in street_hints]) # ignore \"a street\" but keep \"street\"\n",
    "street_phrases_to_ignore.extend([f\"the {word}\" for word in street_hints]) #ignore \"the street\" but keep \"street\"\n",
    "street_phrases_to_ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for traversing the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46e36d",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def glob_enron(path: str,\n",
    "               pattern: str,\n",
    "               a:int=5,\n",
    "               b:int=5,\n",
    "               phrases_to_ignore:Tuple[str]=(),\n",
    "               verbose:bool=False) -> Iterator[Dict]:\n",
    "    \"\"\"\n",
    "    Traverses the Enron dataset and extract subsets of texts with possible addresses\n",
    "    :param path: Path to the enron root folder\n",
    "    :param pattern: The regex pattern to look for\n",
    "    :param a: Number of lines before the pattern match\n",
    "    :param b: Number of lines after the pattern match\n",
    "    :param phrases_to_ignore: List of phrases to scan for each match, and ignore the match if found\n",
    "    :param verbose: True if the function should print to console\n",
    "    :return: Generator\n",
    "    \"\"\"\n",
    "    \n",
    "    for file in glob.glob(f\"{path}/**/[0-9]_\",recursive=True):\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                for match in re.finditer(pattern, line, flags=re.I):\n",
    "                    # adjust a and b if this match is in the first or last rows\n",
    "                    from_line = i-a\n",
    "                    to_line = i+b\n",
    "                    if from_line < 0:\n",
    "                        from_line = 0\n",
    "                    if to_line >= len(lines):\n",
    "                        to_line = len(lines) - 1\n",
    "                    text = \"\".join(lines[from_line : to_line]).replace(\"=09\",\" \").replace(\"=20\",\" \").replace(\"=\\n\",\"\")\n",
    "                    found_phrases = [phrase for phrase in phrases_to_ignore if phrase in lines[i].lower()]\n",
    "                    to_remove = len(found_phrases) > 0\n",
    "                    if not to_remove:\n",
    "                        if verbose:\n",
    "                            print(f\"Found on file file:///{file} on line {i+1} : {match.group()}\") \n",
    "                            print(text)\n",
    "                            print(f\"from:{from_line},to:{to_line}\")\n",
    "                            print(\"---------------------\\n\")\n",
    "                            \n",
    "                        yield {\"file\":file, \"match\": match.group(), \"text\":text}\n",
    "                    else:\n",
    "                        if verbose:\n",
    "                            print(f\"Match {match.group()} removed due to phrases {found_phrases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98686fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = []\n",
    "for match in tqdm(glob_enron(enron_path,\n",
    "                        street_regex, \n",
    "                        phrases_to_ignore=street_phrases_to_ignore,\n",
    "                        verbose=False)):\n",
    "    addresses.append(match)\n",
    "print(f\"Found {len(addresses)} matching records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = list(set([record['text'] for record in addresses]))\n",
    "print(f\"Saving {len(to_save)} unique records\")\n",
    "\n",
    "for i, address_data in enumerate(to_save):\n",
    "    with open(f\"../data/interim/enron_sentences/{i}.txt\",\"w\") as f:\n",
    "        f.writelines(address_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <<<< Annotated records manually with Doccano >>>>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20208bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_dataset = \"../data/processed/enron/dataset.jsonl\"\n",
    "\n",
    "enron_df = pd.read_json(path_or_buf=enron_dataset, lines=True)\n",
    "enron_df = enron_df.sample(frac=1).set_index(\"id\")\n",
    "enron_df = enron_df.drop_duplicates(subset='data', keep=\"last\") #shuffle data and remove duplicates\n",
    "enron_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset size (with and without labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Dataset size: {len(enron_df)}\")\n",
    "no_labels_df = enron_df[enron_df['label'].apply(len)==0]\n",
    "labels_df = enron_df[enron_df['label'].apply(len)>0]\n",
    "print(f\"Len of samples with no labels: {len(no_labels_df)}, number of samples with labels: {len(labels_df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_value(text, labels):\n",
    "    extracted = []\n",
    "    for label in labels:\n",
    "        extracted.append(text[label[0]:label[1]])\n",
    "    return extracted\n",
    "\n",
    "enron_df['values'] = [extract_value(text=row.data,labels=row.label)\n",
    "    for row in enron_df.itertuples()]\n",
    "\n",
    "enron_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All address values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_values = []\n",
    "for val in enron_df['values']:\n",
    "    all_values.extend(val)\n",
    "\n",
    "all_values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "address",
   "language": "python",
   "name": "address"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}